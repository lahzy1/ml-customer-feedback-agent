LLM_CONFIG = {
    "model": "llama3.1:8b",
    "client_host": "127.0.0.1:11434",
    "api_type": "ollama",
    "num_predict": -1,
    "repeat_penalty": 1.1,
    "stream": False,
    "seed": 42,
    "temperature": 1,
    "top_k": 50,
    "top_p": 0.8,
    "native_tool_calls": False,
    "cache_seed": None,
}